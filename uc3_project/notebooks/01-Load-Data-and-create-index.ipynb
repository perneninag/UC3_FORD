{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa60c94-44ca-4254-8a92-7667e6fe8624",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required external libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U mlflow[databricks] lxml==4.9.3 transformers==4.49.0 langchain==0.3.19 databricks-vectorsearch==0.49 bs4==0.0.2 markdownify==0.14.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cce4592-c6d7-4ded-a4f5-92657ab19d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5f5f87-ce8e-499f-8d14-62fa275c4702",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Init our resources and catalog"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e863f82b-f1bf-4047-a189-649bf716b4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import markdownify\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "# Fetch the XML content from sitemap\n",
    "response = requests.get(DATABRICKS_SITEMAP_URL)\n",
    "root = ET.fromstring(response.content)\n",
    "max_documents=None\n",
    "# Find all 'loc' elements (URLs) in the XML\n",
    "urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
    "if max_documents:\n",
    "  urls = urls[:max_documents]\n",
    "\n",
    "    # Create DataFrame from URLs\n",
    "df_urls = spark.createDataFrame(urls, StringType()).toDF(\"url\").repartition(10)\n",
    "df_urls = df_urls.filter(df_urls.url.startswith(\"https://www.ford.com/finance/customer-support\"))\n",
    "#df_urls = df_urls.limit(1)\n",
    "\n",
    "    # Pandas UDF to fetch HTML content for a batch of URLs\n",
    "@pandas_udf(\"string\")\n",
    "def fetch_html_udf(urls: pd.Series) -> pd.Series:\n",
    "  adapter = HTTPAdapter(max_retries=retries)\n",
    "  http = requests.Session()\n",
    "  http.mount(\"http://\", adapter)\n",
    "  http.mount(\"https://\", adapter)\n",
    "  def fetch_html(url):\n",
    "    try:\n",
    "      response = http.get(url)\n",
    "      if response.status_code == 200:\n",
    "        return response.content\n",
    "    except requests.RequestException:\n",
    "      return None\n",
    "    return None\n",
    "\n",
    "  with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "    results = list(executor.map(fetch_html, urls))\n",
    "    return pd.Series(results)\n",
    "\n",
    "    # Pandas UDF to process HTML content and extract text\n",
    "@pandas_udf(\"string\")\n",
    "def download_web_page_udf(html_contents: pd.Series) -> pd.Series:\n",
    "  def extract_text(html_content):\n",
    "    if html_content:\n",
    "      soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "      article = soup.find(\"div\", {\"class\": \"astute-onetopic-result-n\"})\n",
    "      if article:\n",
    "        try:\n",
    "          return markdownify.markdownify(article.prettify(), heading_style=\"ATX\")\n",
    "        except Exception as e:\n",
    "          return None\n",
    "    return None\n",
    "\n",
    "  return html_contents.apply(extract_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddfb6fd-4536-4cc0-b347-fff072579c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # Apply UDFs to DataFrame\n",
    "df_with_html = df_urls.withColumn(\"html_content\", fetch_html_udf(\"url\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecda6b84-ce10-4e94-9bf3-d4a3a862b7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = df_with_html.withColumn(\"text\", download_web_page_udf(\"html_content\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fba0f41-2367-47bc-94b8-2880b4ceb3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"ford_documentation\") or spark.table(\"ford_documentation\").isEmpty():\n",
    "    # Download Databricks documentation to a DataFrame (see _resources/00-init for more details)\n",
    "    doc_articles = download_databricks_documentation_articles()\n",
    "    #Save them as a raw_documentation table\n",
    "    doc_articles.write.mode('overwrite').saveAsTable(\"ford_documentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6f0561-c67f-4785-b467-7826d7969176",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Splitting our html pages in smaller chunks"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n",
    "\n",
    "max_chunk_size = 500\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"##\", \"header2\")])\n",
    "\n",
    "# Split on H2, but merge small h2 chunks together to avoid having too small chunks. \n",
    "def split_html_on_h2(html, min_chunk_size=20, max_chunk_size=500):\n",
    "    if not html:\n",
    "        return []\n",
    "    #removes b64 images captured in the md    \n",
    "    html = re.sub(r'data:image\\/[a-zA-Z]+;base64,[A-Za-z0-9+/=\\n]+', '', html, flags=re.MULTILINE)\n",
    "    chunks = []\n",
    "    previous_chunk = \"\"\n",
    "    for c in md_splitter.split_text(html):\n",
    "        content = c.metadata.get('header2', \"\") + \"\\n\" + c.page_content\n",
    "        if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size / 2:\n",
    "            previous_chunk += content + \"\\n\"\n",
    "        else:\n",
    "            chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "            previous_chunk = content + \"\\n\"\n",
    "    if previous_chunk:\n",
    "        chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    "\n",
    "# Let's try our chunking function\n",
    "html = spark.table(\"ford_documentation\").limit(1).collect()[0]['text']\n",
    "split_html_on_h2(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0685b623-481b-432e-a2a7-732124945446",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the final databricks_documentation table containing chunks"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS databricks_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embeddings STRING\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29126373-5848-4fd5-b210-b9567f8e8d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a user-defined function (UDF) to chunk all our documents with spark\n",
    "@pandas_udf(\"array<string>\")\n",
    "def parse_and_split(docs: pd.Series) -> pd.Series:\n",
    "    return docs.apply(split_html_on_h2)\n",
    "    \n",
    "(spark.table(\"ford_documentation\")\n",
    "      .filter('text is not null')\n",
    "      .repartition(30)\n",
    "      .withColumn('content', F.explode(parse_and_split('text')))\n",
    "      .drop(\"text\")\n",
    "      .write.mode('overwrite').saveAsTable(\"databricks_documentation\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a69c11-b62b-4191-a778-3437a6b437bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS support_doc_embeddings_v2 (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embeddings ARRAY<STRING>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6e114f-9a2a-4b2a-b688-c94f7954b183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import array\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "def get_embeddings(input: pd.Series):\n",
    "    response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [input]})\n",
    "    embeddings = [e['embedding'] for e in response.data]\n",
    "    return input\n",
    "# Register the Python function as a UDF\n",
    "getEmbeddings = F.udf(\n",
    "    lambda content: content,#get_embeddings(content),\n",
    "    \"array<double>\",\n",
    ")\n",
    "docs=spark.table(\"databricks_documentation\").withColumn(\"embeddings\", getEmbeddings(\"content\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a92d19a-96d9-477d-ab09-6e8d1ed3bc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings(input):\n",
    "    response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [str(input)]})\n",
    "    embeddings = [str(e['embedding']) for e in response.data]\n",
    "    return embeddings\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def test(input: pd.Series) -> pd.Series:\n",
    "    result=input.apply(get_embeddings)\n",
    "    return result\n",
    "\n",
    "docs=spark.table(\"databricks_documentation\").withColumn(\"embeddings\", test(\"content\"))\n",
    "docs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5919a6f-d1e8-47dc-9e7a-5376034998ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs.write.mode('overwrite').saveAsTable(\"support_doc_embeddings_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ab7d73-cfb7-4b2e-96c4-d247d3b7f449",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "What is an embedding"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "#Embeddings endpoints convert text into a vector (array of float). Here is an example using GTEgte:\n",
    "response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "print(embeddings)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58a73fd-e18e-46cb-990a-9236719cd21a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ce9b3-0cfe-43f6-ac87-7b0c3c60608e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the managed vector search using our endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.databricks_documentation\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.databricks_documentation_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  try:\n",
    "    vsc.create_delta_sync_index(\n",
    "      endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "      index_name=vs_index_fullname,\n",
    "      source_table_name=source_table_fullname,\n",
    "      pipeline_type=\"TRIGGERED\",\n",
    "      primary_key=\"id\",\n",
    "      embedding_source_column='content', #The column containing our text\n",
    "      embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "    )\n",
    "  except Exception as e:\n",
    "    display_quota_error(e, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "    raise e\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992543c5-0987-473f-9f64-c30b9a03a805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Searching for similar content\n",
    "\n",
    "That's all we have to do. Databricks will automatically capture and synchronize new entries in your Delta Live Table.\n",
    "\n",
    "Note that depending on your dataset size and model size, index creation can take a few seconds to start and index your embeddings.\n",
    "\n",
    "Let's give it a try and search for similar content.\n",
    "\n",
    "*Note: `similarity_search` also support a filters parameter. This is useful to add a security layer to your RAG system: you can filter out some sensitive content based on who is doing the call (for example filter on a specific department based on the user preference).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75616130-45f8-4156-9a70-2504c42fcac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2dcf7b8-4432-45e2-abf4-f5d76567682f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next step: Deploy our chatbot model with RAG using DBRX\n",
    "\n",
    "We've seen how Databricks Lakehouse AI makes it easy to ingest and prepare your documents, and deploy a Vector Search index on top of it with just a few lines of code and configuration.\n",
    "\n",
    "This simplifies and accelerates your data projects so that you can focus on the next step: creating your real-time chatbot endpoint with well-crafted prompt augmentation.\n",
    "\n",
    "Open the [02-Deploy-RAG-Chatbot-Model]($./02-Deploy-RAG-Chatbot-Model) notebook to create and deploy a chatbot endpoint."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3733658022851874,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Load-Data-and-create-index",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
